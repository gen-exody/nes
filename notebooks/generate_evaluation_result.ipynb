{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c41cdb8",
   "metadata": {},
   "source": [
    "# 8. Generate Evaluation Result\n",
    "\n",
    "We have defined 9 queries to search over the evaluation dataset (products with 11-14 reviews). The output was then rated by 3 raters with a scale of 1 to 5 where 1 denoted “Not relevant at all” while 5 denoted “Perfectly relevant”. \n",
    "\n",
    "We use [Normalized Discounted Cumulative Gain (NDCG)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to evaluate the goodness of ranking for our search engine under the 3 ranking methods, i.e. *i) Average*, *ii) Discounted Reward*, and *iii) Discounted Reward with Adjustment by Opposite Query*. The *i) Average* method acts as the baseline for our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8302cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import krippendorff\n",
    "from sklearn.metrics import ndcg_score\n",
    "import pingouin as pg\n",
    "import session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05578676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paths for files containing product-level ratings from 1-5, 5 being the most relevant.\n",
    "u1_rating_file_path='../resources/eval/Ratings_U1/*.xlsx'\n",
    "u2_rating_file_path='../resources/eval/Ratings_U2/*.xlsx'\n",
    "u3_rating_file_path='../resources/eval/Ratings_U3.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce2c92",
   "metadata": {},
   "source": [
    "## Calculating NDCG\n",
    "\n",
    "For the details on how NDCG is calculated, we recommend this article [“Demystifying NDCG” by Aparna Dhinakaran](https://towardsdatascience.com/demystifying-ndcg-bee3be58cfe0).   \n",
    "Here for our implementation, we use the `ndcg_score` function from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce60c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Normalized Discounted Cumulative Gain for each query, each ranking method, and each user.\n",
    "def NDCG(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path):\n",
    "    \n",
    "    # Create file lists for Users 1 and 2 since they have similar rating file formats.\n",
    "    u1_file_paths=glob.glob(u1_rating_file_path)\n",
    "    u2_file_paths=glob.glob(u2_rating_file_path)\n",
    "    file_paths=[u1_file_paths, u2_file_paths]\n",
    "    user_df_dict={}\n",
    "    \n",
    "    # Loop through each file in the folders for Users 1 and 2.\n",
    "    for file_path in file_paths:\n",
    "        ndcg_dict={}\n",
    "        for path in file_path:\n",
    "            \n",
    "            # Read each rating file as a dataframe and perform basic sanity checks.\n",
    "            df=pd.read_excel(path)\n",
    "            df.dropna(subset=[\"Query ID\", \"User ID\"], inplace=True)\n",
    "            # Ensure that each line item has product-level rating.\n",
    "            df=df.loc[~(df[\"Detailed Rating (product level)\"].isna())|(~(df[\"Relevance Indicator\"].isna()))]\n",
    "            # Ensure that the relevance indicator is present for at least one product for a given query and ranking method.\n",
    "            df.loc[(df[\"product_id\"].duplicated(keep=False))&(~(df[\"Detailed Rating (product level)\"].isna())), \"Relevance Indicator\"]=\"Most relevant\"\n",
    "            df.dropna(subset=[\"Detailed Rating (product level)\"], inplace=True)\n",
    "            \n",
    "            # Calculate the ideal rank\n",
    "            df[\"ideal_rating\"]=sorted(list(df[\"Detailed Rating (product level)\"]), reverse=True)\n",
    "  \n",
    "            # Calculate the NDCG\n",
    "            ndcg_10 = ndcg_score(df['ideal_rating'].values.reshape(1,-1), df['Detailed Rating (product level)'].values.reshape(1,-1), k=10)\n",
    "            ndcg_5 = ndcg_score(df['ideal_rating'].values.reshape(1,-1), df['Detailed Rating (product level)'].values.reshape(1,-1), k=5)\n",
    "            ndcg_3 = ndcg_score(df['ideal_rating'].values.reshape(1,-1), df['Detailed Rating (product level)'].values.reshape(1,-1), k=3)\n",
    "            \n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            if \"Query ID\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"Query ID\"]=[df[\"Query ID\"].iloc[0]]\n",
    "            else:\n",
    "                ndcg_dict[\"Query ID\"].append(df[\"Query ID\"].iloc[0])\n",
    "            if \"Ranking Method\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"Ranking Method\"]=[df[\"Ranking Method\"].iloc[0]]\n",
    "            else:\n",
    "                ndcg_dict[\"Ranking Method\"].append(df[\"Ranking Method\"].iloc[0])\n",
    "            if \"User ID\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"User ID\"]=[df[\"User ID\"].iloc[0]]\n",
    "            else:\n",
    "                ndcg_dict[\"User ID\"].append(df[\"User ID\"].iloc[0])\n",
    "            if \"NDCG@10\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"NDCG@10\"]=[ndcg_10]\n",
    "            else:\n",
    "                ndcg_dict[\"NDCG@10\"].append(ndcg_10)\n",
    "            if \"NDCG@5\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"NDCG@5\"]=[ndcg_5]\n",
    "            else:\n",
    "                ndcg_dict[\"NDCG@5\"].append(ndcg_5)\n",
    "            if \"NDCG@3\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"NDCG@3\"]=[ndcg_3]\n",
    "            else:\n",
    "                ndcg_dict[\"NDCG@3\"].append(ndcg_3)\n",
    "\n",
    "        # Convert the dictionary to a dataframe and tidy it up.\n",
    "        u_df=pd.DataFrame(ndcg_dict)\n",
    "        u_df1=u_df.loc[u_df[\"Query ID\"]!=\"Q10\"].copy()\n",
    "        u_df2=u_df.loc[u_df[\"Query ID\"]==\"Q10\"].copy()\n",
    "        u_df=pd.concat([u_df1, u_df2])\n",
    "        u_df.reset_index(drop=True, inplace=True)\n",
    "        # Add each user's result dataframe as a value in a dictionary.\n",
    "        user_df_dict[df[\"User ID\"].iloc[0]]=u_df\n",
    "    \n",
    "    # Create a list of query IDs.\n",
    "    queries=list(u_df[\"Query ID\"].unique())\n",
    "    \n",
    "    # Loop through each query ID for User 3, whose rating file differs from those of Users 1 and 2.\n",
    "    ndcg_dict={}\n",
    "    for query in queries:\n",
    "        \n",
    "        # Read each sheet in the rating file as a dataframe and perform basic sanity checks.\n",
    "        df=pd.read_excel(u3_rating_file_path, sheet_name=query)\n",
    "        df.dropna(subset=[\"product_id\",\"review_id\"], inplace=True)\n",
    "        \n",
    "        # Separate the dataframe based on ranking method for ease of calculation.\n",
    "        dfa=df.loc[df[\"Ranking Method\"]==\"Average\"].copy()\n",
    "        dfd=df.loc[df[\"Ranking Method\"]==\"Discounted Reward Only\"].copy()\n",
    "        dfda=df.loc[df[\"Ranking Method\"]==\"Discounted Reward with Adjustment\"].copy()\n",
    "\n",
    "        # Create a list of the dataframes corresponding to each ranking method.\n",
    "        df_list=[dfa, dfd, dfda]\n",
    "\n",
    "        # Loop through each dataframe in the list corresponding to a different ranking method.\n",
    "        for dfx in df_list:\n",
    "            # Calculate the ideal rank\n",
    "            dfx[\"ideal_rating\"]=sorted(list(dfx[\"Detailed Rating (product level)\"]), reverse=True)\n",
    "\n",
    "            # Calculate the NDCG\n",
    "            ndcg_10 = ndcg_score(dfx['ideal_rating'].values.reshape(1,-1), dfx['Detailed Rating (product level)'].values.reshape(1,-1), k=10)\n",
    "            ndcg_5 = ndcg_score(dfx['ideal_rating'].values.reshape(1,-1), dfx['Detailed Rating (product level)'].values.reshape(1,-1), k=5)\n",
    "            ndcg_3 = ndcg_score(dfx['ideal_rating'].values.reshape(1,-1), dfx['Detailed Rating (product level)'].values.reshape(1,-1), k=3)\n",
    "\n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            if \"Query ID\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"Query ID\"]=[dfx[\"Query ID\"].iloc[0]]\n",
    "            else:\n",
    "                ndcg_dict[\"Query ID\"].append(dfx[\"Query ID\"].iloc[0])\n",
    "            if \"Ranking Method\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"Ranking Method\"]=[dfx[\"Ranking Method\"].iloc[0]]\n",
    "            else:\n",
    "                ndcg_dict[\"Ranking Method\"].append(dfx[\"Ranking Method\"].iloc[0])\n",
    "            if \"User ID\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"User ID\"]=[dfx[\"User ID\"].iloc[0]]\n",
    "            else:\n",
    "                ndcg_dict[\"User ID\"].append(dfx[\"User ID\"].iloc[0])\n",
    "            if \"NDCG@10\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"NDCG@10\"]=[ndcg_10]\n",
    "            else:\n",
    "                ndcg_dict[\"NDCG@10\"].append(ndcg_10)\n",
    "            if \"NDCG@5\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"NDCG@5\"]=[ndcg_5]\n",
    "            else:\n",
    "                ndcg_dict[\"NDCG@5\"].append(ndcg_5)\n",
    "            if \"NDCG@3\" not in ndcg_dict.keys():\n",
    "                ndcg_dict[\"NDCG@3\"]=[ndcg_3]\n",
    "            else:\n",
    "                ndcg_dict[\"NDCG@3\"].append(ndcg_3)\n",
    "\n",
    "    # Convert the dictionary to a dataframe and tidy it up.\n",
    "    u_df=pd.DataFrame(ndcg_dict)\n",
    "    # Add User 3's result dataframe as a value in the dictionary that contains the result dataframes for Users 1 and 2.\n",
    "    user_df_dict[u_df[\"User ID\"].iloc[0]]=u_df\n",
    "    \n",
    "    # Concatenate all three users' result dataframes into one big dataframe.\n",
    "    user_df=pd.DataFrame()\n",
    "    for df in user_df_dict.keys():\n",
    "        user_df=pd.concat([user_df, user_df_dict[df]])\n",
    "\n",
    "    # Return the dictionary containing each user's result dataframes and the big dataframe containing all 3 users' NDCG results.\n",
    "    return user_df_dict, user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc5e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndcg_by_users, df_ndcg = NDCG(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b6734d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query ID</th>\n",
       "      <th>Ranking Method</th>\n",
       "      <th>User ID</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Average</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.926717</td>\n",
       "      <td>0.855328</td>\n",
       "      <td>0.796736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Discounted Reward Only</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.946090</td>\n",
       "      <td>0.838319</td>\n",
       "      <td>0.856825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Discounted Reward with Adjustment</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.945335</td>\n",
       "      <td>0.824223</td>\n",
       "      <td>0.821877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Average</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.938423</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Discounted Reward Only</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.938423</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Discounted Reward with Adjustment</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.933606</td>\n",
       "      <td>0.820920</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q3</td>\n",
       "      <td>Average</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.963934</td>\n",
       "      <td>0.922161</td>\n",
       "      <td>0.898368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q3</td>\n",
       "      <td>Discounted Reward Only</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.958255</td>\n",
       "      <td>0.901201</td>\n",
       "      <td>0.887982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q3</td>\n",
       "      <td>Discounted Reward with Adjustment</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.957288</td>\n",
       "      <td>0.897008</td>\n",
       "      <td>0.885905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q4</td>\n",
       "      <td>Average</td>\n",
       "      <td>U2</td>\n",
       "      <td>0.904810</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.711756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Query ID                     Ranking Method User ID   NDCG@10    NDCG@5  \\\n",
       "0       Q1                            Average      U2  0.926717  0.855328   \n",
       "1       Q1             Discounted Reward Only      U2  0.946090  0.838319   \n",
       "2       Q1  Discounted Reward with Adjustment      U2  0.945335  0.824223   \n",
       "3       Q2                            Average      U2  0.938423  0.833333   \n",
       "4       Q2             Discounted Reward Only      U2  0.938423  0.833333   \n",
       "5       Q2  Discounted Reward with Adjustment      U2  0.933606  0.820920   \n",
       "6       Q3                            Average      U2  0.963934  0.922161   \n",
       "7       Q3             Discounted Reward Only      U2  0.958255  0.901201   \n",
       "8       Q3  Discounted Reward with Adjustment      U2  0.957288  0.897008   \n",
       "9       Q4                            Average      U2  0.904810  0.800183   \n",
       "\n",
       "     NDCG@3  \n",
       "0  0.796736  \n",
       "1  0.856825  \n",
       "2  0.821877  \n",
       "3  0.833333  \n",
       "4  0.833333  \n",
       "5  0.833333  \n",
       "6  0.898368  \n",
       "7  0.887982  \n",
       "8  0.885905  \n",
       "9  0.711756  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample output\n",
    "df_ndcg_by_users['U2'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24337fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line if you want to save the result\n",
    "#df_ndcg.to_csv('../resources/eval/ndcg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1b1e4",
   "metadata": {},
   "source": [
    "Refer to the notebook `evaluation_analysis.ipynb` for further analysis on NDCG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6280a",
   "metadata": {},
   "source": [
    "## Calculate Mean Reciprocal Rank (MRR)\n",
    "\n",
    "MRR is another measurement of the goodness of ranking by measuring how far down the ranking the first relevant document is.  \n",
    "For details, please visit this article - [Compute Mean Reciprocal Rank (MRR) using Pandas](https://softwaredoug.com/blog/2021/04/21/compute-mrr-using-pandas.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bafe85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Mean Reciprocal Rank for each ranking method and each user.\n",
    "def MRR(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path):\n",
    "    \n",
    "    # Create file lists for Users 1 and 2 since they have similar rating file formats.\n",
    "    u1_file_paths=glob.glob(u1_rating_file_path)\n",
    "    u2_file_paths=glob.glob(u2_rating_file_path)\n",
    "    file_paths=[u1_file_paths, u2_file_paths]\n",
    "    user_df_dict={}\n",
    "    \n",
    "    # Loop through each file in the folders for Users 1 and 2.\n",
    "    for file_path in file_paths:\n",
    "        mrr_dict={}\n",
    "        for path in file_path:\n",
    "            \n",
    "            # Read each rating file as a dataframe and perform basic sanity checks.\n",
    "            df=pd.read_excel(path)\n",
    "            df.dropna(subset=[\"Query ID\", \"User ID\"], inplace=True)\n",
    "            # Ensure that each line item has product-level rating.\n",
    "            df=df.loc[~(df[\"Detailed Rating (product level)\"].isna())|(~(df[\"Relevance Indicator\"].isna()))]\n",
    "            # Ensure that the relevance indicator is present for at least one product for a given query and ranking method.\n",
    "            df.loc[(df[\"product_id\"].duplicated(keep=False))&(~(df[\"Detailed Rating (product level)\"].isna())), \"Relevance Indicator\"]=\"Most relevant\"\n",
    "            df.dropna(subset=[\"Detailed Rating (product level)\"], inplace=True)\n",
    "            \n",
    "            # Calculate the rank, the reciprocal rank, and the reciprocal rank of the most relevant product.\n",
    "            df[\"rank\"]=list(range(1, len(df)+1))\n",
    "            df[\"reciprocal_rank\"]=round(1/df[\"rank\"], 3)\n",
    "            rr=df.loc[df[\"Relevance Indicator\"]==\"Most relevant\", \"reciprocal_rank\"].iloc[0]\n",
    "            \n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            if \"Query ID\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"Query ID\"]=[df[\"Query ID\"].iloc[0]]\n",
    "            else:\n",
    "                mrr_dict[\"Query ID\"].append(df[\"Query ID\"].iloc[0])\n",
    "            if \"Ranking Method\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"Ranking Method\"]=[df[\"Ranking Method\"].iloc[0]]\n",
    "            else:\n",
    "                mrr_dict[\"Ranking Method\"].append(df[\"Ranking Method\"].iloc[0])\n",
    "            if \"User ID\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"User ID\"]=[df[\"User ID\"].iloc[0]]\n",
    "            else:\n",
    "                mrr_dict[\"User ID\"].append(df[\"User ID\"].iloc[0])\n",
    "            if \"Reciprocal Rank\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"Reciprocal Rank\"]=[rr]\n",
    "            else:\n",
    "                mrr_dict[\"Reciprocal Rank\"].append(rr)\n",
    "        \n",
    "        # Convert the dictionary to a dataframe and tidy it up.\n",
    "        u_df=pd.DataFrame(mrr_dict)\n",
    "        u_df1=u_df.loc[u_df[\"Query ID\"]!=\"Q10\"].copy()\n",
    "        u_df2=u_df.loc[u_df[\"Query ID\"]==\"Q10\"].copy()\n",
    "        u_df=pd.concat([u_df1, u_df2])\n",
    "        u_df.reset_index(drop=True, inplace=True)\n",
    "        # Add each user's result dataframe as a value in a dictionary.\n",
    "        user_df_dict[df[\"User ID\"].iloc[0]]=u_df\n",
    "        \n",
    "    # Create a list of query IDs.\n",
    "    queries=list(u_df[\"Query ID\"].unique())\n",
    "    \n",
    "    # Loop through each query ID for User 3, whose rating file format differs from those of Users 1 and 2.\n",
    "    mrr_dict={}\n",
    "    for query in queries:\n",
    "        \n",
    "        # Read each sheet in the rating file as a dataframe and perform basic sanity checks.\n",
    "        df=pd.read_excel(u3_rating_file_path, sheet_name=query)\n",
    "        df.dropna(subset=[\"product_id\",\"review_id\"], inplace=True)\n",
    "\n",
    "        # Separate the dataframe based on ranking method for ease of calculation.\n",
    "        dfa=df.loc[df[\"Ranking Method\"]==\"Average\"].copy()\n",
    "        dfd=df.loc[df[\"Ranking Method\"]==\"Discounted Reward Only\"].copy()\n",
    "        dfda=df.loc[df[\"Ranking Method\"]==\"Discounted Reward with Adjustment\"].copy()\n",
    "\n",
    "        # Create a list of the dataframes corresponding to each ranking method.\n",
    "        df_list=[dfa, dfd, dfda]\n",
    "\n",
    "        # Loop through each dataframe in the list corresponding to a different ranking method.\n",
    "        for dfx in df_list:\n",
    "            \n",
    "            # Calculate the rank, the reciprocal rank, and the reciprocal rank of the most relevant product.           \n",
    "            dfx[\"rank\"]=list(range(1, len(dfx)+1))\n",
    "            dfx[\"reciprocal_rank\"]=round(1/dfx[\"rank\"], 3)\n",
    "            rr=dfx.loc[dfx[\"Relevance Indicator\"]==\"Most relevant\", \"reciprocal_rank\"].iloc[0]\n",
    "            \n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            if \"Query ID\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"Query ID\"]=[dfx[\"Query ID\"].iloc[0]]\n",
    "            else:\n",
    "                mrr_dict[\"Query ID\"].append(dfx[\"Query ID\"].iloc[0])\n",
    "            if \"Ranking Method\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"Ranking Method\"]=[dfx[\"Ranking Method\"].iloc[0]]\n",
    "            else:\n",
    "                mrr_dict[\"Ranking Method\"].append(dfx[\"Ranking Method\"].iloc[0])\n",
    "            if \"User ID\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"User ID\"]=[dfx[\"User ID\"].iloc[0]]\n",
    "            else:\n",
    "                mrr_dict[\"User ID\"].append(dfx[\"User ID\"].iloc[0])\n",
    "            if \"Reciprocal Rank\" not in mrr_dict.keys():\n",
    "                mrr_dict[\"Reciprocal Rank\"]=[rr]\n",
    "            else:\n",
    "                mrr_dict[\"Reciprocal Rank\"].append(rr)\n",
    "                \n",
    "    # Convert the dictionary to a dataframe and tidy it up.\n",
    "    u_df=pd.DataFrame(mrr_dict)\n",
    "    # Add User 3's result dataframe as a value in the dictionary that contains the result dataframes for Users 1 and 2.\n",
    "    user_df_dict[u_df[\"User ID\"].iloc[0]]=u_df\n",
    "    \n",
    "    # Concatenate all three users' result dataframes into one big dataframe.\n",
    "    user_df=pd.DataFrame()\n",
    "    for df in user_df_dict.keys():\n",
    "        user_df=pd.concat([user_df, user_df_dict[df]])\n",
    "    \n",
    "    # Calculate the MRR after grouping the result by the Ranking Method for each user.\n",
    "    u1_ranks=user_df.loc[user_df[\"User ID\"]==\"U1\"].copy()\n",
    "    u2_ranks=user_df.loc[user_df[\"User ID\"]==\"U2\"].copy()\n",
    "    u3_ranks=user_df.loc[user_df[\"User ID\"]==\"U3\"].copy()\n",
    "    u1_mrr=u1_ranks.groupby([\"Ranking Method\"]).agg({\"Reciprocal Rank\": \"mean\"}).rename(columns={\"Reciprocal Rank\": \"MRR\"}).reset_index()\n",
    "    u2_mrr=u2_ranks.groupby([\"Ranking Method\"]).agg({\"Reciprocal Rank\": \"mean\"}).rename(columns={\"Reciprocal Rank\": \"MRR\"}).reset_index()\n",
    "    u3_mrr=u3_ranks.groupby([\"Ranking Method\"]).agg({\"Reciprocal Rank\": \"mean\"}).rename(columns={\"Reciprocal Rank\": \"MRR\"}).reset_index()\n",
    "    u1_mrr[\"User ID\"]=\"U1\"\n",
    "    u2_mrr[\"User ID\"]=\"U2\"\n",
    "    u3_mrr[\"User ID\"]=\"U3\"\n",
    "    \n",
    "    # Combine the MRR result dataframe of all three users.\n",
    "    ranks_df=pd.concat([u1_mrr, u2_mrr, u3_mrr])\n",
    "            \n",
    "    # Return the dictionary containing each user's result dataframes and the dataframes containing all 3 users' MRR results.\n",
    "    return user_df_dict, user_df, ranks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74c3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrr_by_users, df_mrr, df_mrr_ranks = MRR(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110cf803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking Method</th>\n",
       "      <th>MRR</th>\n",
       "      <th>User ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.568444</td>\n",
       "      <td>U1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discounted Reward Only</td>\n",
       "      <td>0.401778</td>\n",
       "      <td>U1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discounted Reward with Adjustment</td>\n",
       "      <td>0.612111</td>\n",
       "      <td>U1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.394333</td>\n",
       "      <td>U2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discounted Reward Only</td>\n",
       "      <td>0.351333</td>\n",
       "      <td>U2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discounted Reward with Adjustment</td>\n",
       "      <td>0.584333</td>\n",
       "      <td>U2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.494333</td>\n",
       "      <td>U3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discounted Reward Only</td>\n",
       "      <td>0.329222</td>\n",
       "      <td>U3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discounted Reward with Adjustment</td>\n",
       "      <td>0.490667</td>\n",
       "      <td>U3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Ranking Method       MRR User ID\n",
       "0                            Average  0.568444      U1\n",
       "1             Discounted Reward Only  0.401778      U1\n",
       "2  Discounted Reward with Adjustment  0.612111      U1\n",
       "0                            Average  0.394333      U2\n",
       "1             Discounted Reward Only  0.351333      U2\n",
       "2  Discounted Reward with Adjustment  0.584333      U2\n",
       "0                            Average  0.494333      U3\n",
       "1             Discounted Reward Only  0.329222      U3\n",
       "2  Discounted Reward with Adjustment  0.490667      U3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample result\n",
    "df_mrr_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26616bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line if you want to save the result\n",
    "#df_mrr_ranks.to_csv('../resources/eval/mrr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1ca871",
   "metadata": {},
   "source": [
    "Refer to the notebook `evaluation_analysis.ipynb` for further analysis on MRR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6d87a",
   "metadata": {},
   "source": [
    "## Calculate Krippendorff's alpha\n",
    "\n",
    "Krippendorff's alpha is used to evaluate inter-rater reliability. Where Cohen's kappa measures are used for the same purpose but in cases where only two raters provide ratings, Krippendorff's alpha can be conveniently used in the case of multiple raters.\n",
    "\n",
    "For details, please visit this article - [Inter-Annotator Agreement: An Introduction to Krippendorff’s Alpha](https://www.surgehq.ai/blog/inter-rater-reliability-metrics-an-introduction-to-krippendorffs-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b927d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate. Krippendorff's alpha to evaluate inter-rater reliability\n",
    "def Krippendorff_alpha(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path):\n",
    "    \n",
    "    # Create file lists for Users 1 and 2 since they have similar rating file formats.\n",
    "    u1_file_paths=glob.glob(u1_rating_file_path)\n",
    "    u2_file_paths=glob.glob(u2_rating_file_path)\n",
    "    file_paths=[u1_file_paths, u2_file_paths]\n",
    "    user_df_dict={}\n",
    "    \n",
    "    # Loop through each file in the folders for Users 1 and 2.\n",
    "    user_dict={}\n",
    "    for file_path in file_paths:\n",
    "        kripp_dict={}\n",
    "        for path in file_path:\n",
    "            \n",
    "            # Read each rating file as a dataframe and perform basic sanity checks.\n",
    "            df=pd.read_excel(path)\n",
    "            df.dropna(subset=[\"Query ID\", \"User ID\"], inplace=True)\n",
    "            # Ensure that each line item has product-level rating.\n",
    "            df=df.loc[~(df[\"Detailed Rating (product level)\"].isna())|(~(df[\"Relevance Indicator\"].isna()))]\n",
    "            # Ensure that the relevance indicator is present for at least one product for a given query and ranking method.\n",
    "            df.loc[(df[\"product_id\"].duplicated(keep=False))&(~(df[\"Detailed Rating (product level)\"].isna())), \"Relevance Indicator\"]=\"Most relevant\"\n",
    "            df.dropna(subset=[\"Detailed Rating (product level)\"], inplace=True)\n",
    "            \n",
    "            # Keep relevant columns.\n",
    "            df=df.loc[:,[\"Query ID\", \"Ranking Method\", \"product_id\", \"User ID\", \"Detailed Rating (product level)\"]]\n",
    "            # Create a new column which is a string combination of Query ID, Ranking Method and product_id.\n",
    "            df[\"Query ID__Ranking Method__product_id\"]=df[\"Query ID\"]+\"_\"+df[\"Ranking Method\"]+\"_\"+df[\"product_id\"]\n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            kripp_dict[df[\"User ID\"].iloc[0]+\"_\"+df[\"Query ID\"].iloc[0]+\"_\"+df[\"Ranking Method\"].iloc[0]]=df\n",
    "            \n",
    "        # Add each user's dictionary as a value in a dictionary, creating a nested dictionary.\n",
    "        user_dict[df[\"User ID\"].iloc[0]]=kripp_dict\n",
    "    \n",
    "    # Create a list of query IDs.\n",
    "    queries=[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\"]\n",
    "    \n",
    "    # Loop through each query ID for User 3, whose rating file format differs from those of Users 1 and 2.\n",
    "    kripp_dict={}\n",
    "    for query in queries:\n",
    "        \n",
    "        # Read each sheet in the rating file as a dataframe and perform basic sanity checks.\n",
    "        df=pd.read_excel(u3_rating_file_path, sheet_name=query)\n",
    "        df.dropna(subset=[\"product_id\",\"review_id\"], inplace=True)\n",
    "\n",
    "        # Separate the dataframe based on ranking method for ease of calculation.\n",
    "        dfa=df.loc[df[\"Ranking Method\"]==\"Average\"].copy()\n",
    "        dfd=df.loc[df[\"Ranking Method\"]==\"Discounted Reward Only\"].copy()\n",
    "        dfda=df.loc[df[\"Ranking Method\"]==\"Discounted Reward with Adjustment\"].copy()\n",
    "\n",
    "        # Create a list of the dataframes corresponding to each ranking method.\n",
    "        df_list=[dfa, dfd, dfda]\n",
    "\n",
    "        # Loop through each dataframe in the list corresponding to a different ranking method.\n",
    "        for dfx in df_list:\n",
    "            \n",
    "            # Keep relevant columns.\n",
    "            dfx=dfx.loc[:,[\"Query ID\", \"Ranking Method\", \"product_id\", \"User ID\", \"Detailed Rating (product level)\"]]\n",
    "            # Create a new column which is a string combination of Query ID, Ranking Method and product_id.\n",
    "            dfx[\"Query ID__Ranking Method__product_id\"]=dfx[\"Query ID\"]+\"_\"+dfx[\"Ranking Method\"]+\"_\"+dfx[\"product_id\"]\n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            kripp_dict[dfx[\"User ID\"].iloc[0]+\"_\"+dfx[\"Query ID\"].iloc[0]+\"_\"+dfx[\"Ranking Method\"].iloc[0]]=dfx\n",
    "    \n",
    "    # Add User 3's dictionary as a value in the dictionary that contains the result dictionaries for Users 1 and 2.\n",
    "    user_dict[dfx[\"User ID\"].iloc[0]]=kripp_dict\n",
    "    \n",
    "    # Combine all 3 users' dataframes by querying the nested dictionary.\n",
    "    main_df=pd.DataFrame()\n",
    "    for user in user_dict.keys():\n",
    "        for df in user_dict[user]:\n",
    "            dfx=user_dict[user][df]\n",
    "            main_df=pd.concat([main_df, dfx])\n",
    "    \n",
    "    # Create a list of each user's ratings for Krippendorff's alpha calculation.\n",
    "    u1_ratings_df=main_df.loc[main_df[\"User ID\"]==\"U1\"].copy()\n",
    "    u2_ratings_df=main_df.loc[main_df[\"User ID\"]==\"U2\"].copy()\n",
    "    u3_ratings_df=main_df.loc[main_df[\"User ID\"]==\"U3\"].copy()\n",
    "    u1_ratings_list=list(u1_ratings_df[\"Detailed Rating (product level)\"])\n",
    "    u2_ratings_list=list(u2_ratings_df[\"Detailed Rating (product level)\"])\n",
    "    u3_ratings_list=list(u3_ratings_df[\"Detailed Rating (product level)\"])\n",
    "        \n",
    "    # Create a list of lists, with each sublist corresponding to the ratings of one user.\n",
    "    ratings_list=[u1_ratings_list, u2_ratings_list, u3_ratings_list]\n",
    "\n",
    "    # Calculate Krippendorff's alpha.\n",
    "    alpha=round(krippendorff.alpha(reliability_data=ratings_list, level_of_measurement=\"ordinal\"), 3)\n",
    "    \n",
    "    # Return the Krippendorff's alpha value.\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c430d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=Krippendorff_alpha(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "305125e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.585"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e7289",
   "metadata": {},
   "source": [
    "Krippendorff's alpha is a quantitative measure based on the observed disagreement of raters corrected for disagreement expected by chance. The range of this measure lies between -1 and 1, where 1 indicates perfect agreement, 0 indicates no agreement beyond chance, and negative values indicate inverse agreement.\n",
    "\n",
    "Our Krippendorff's alpha value of 0.586 indicates above average agreement between the three raters using ordinal evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6235785",
   "metadata": {},
   "source": [
    "## Calculate Intra-Class Correlation (ICC)\n",
    "\n",
    "The Intra-Class correlation (ICC) is another quantitative measure to assess the reliability of ratings by multiple subjects. The measure compares the variability of different ratings of the same subject to the total variation across all ratings and all subjects.\n",
    "\n",
    "The [documentation of the pingouin library](https://pingouin-stats.org/build/html/generated/pingouin.intraclass_corr.html), which contains the Python implementation of ICC, can be referred to for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1c6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate. Krippendorff's alpha to evaluate inter-rater reliability\n",
    "def ICC(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path):\n",
    "    \n",
    "    # Create file lists for Users 1 and 2 since they have similar rating file formats.\n",
    "    u1_file_paths=glob.glob(u1_rating_file_path)\n",
    "    u2_file_paths=glob.glob(u2_rating_file_path)\n",
    "    file_paths=[u1_file_paths, u2_file_paths]\n",
    "    user_df_dict={}\n",
    "    \n",
    "    # Loop through each file in the folders for Users 1 and 2.\n",
    "    user_dict={}\n",
    "    for file_path in file_paths:\n",
    "        icc_dict={}\n",
    "        for path in file_path:\n",
    "            \n",
    "            # Read each rating file as a dataframe and perform basic sanity checks.\n",
    "            df=pd.read_excel(path)\n",
    "            df.dropna(subset=[\"Query ID\", \"User ID\"], inplace=True)\n",
    "            # Ensure that each line item has product-level rating.\n",
    "            df=df.loc[~(df[\"Detailed Rating (product level)\"].isna())|(~(df[\"Relevance Indicator\"].isna()))]\n",
    "            # Ensure that the relevance indicator is present for at least one product for a given query and ranking method.\n",
    "            df.loc[(df[\"product_id\"].duplicated(keep=False))&(~(df[\"Detailed Rating (product level)\"].isna())), \"Relevance Indicator\"]=\"Most relevant\"\n",
    "            df.dropna(subset=[\"Detailed Rating (product level)\"], inplace=True)\n",
    "            \n",
    "            # Keep relevant columns.\n",
    "            df=df.loc[:,[\"Query ID\", \"Ranking Method\", \"product_id\", \"User ID\", \"Detailed Rating (product level)\"]]\n",
    "            # Create a new column which is a string combination of Query ID, Ranking Method and product_id.\n",
    "            df[\"Query ID__Ranking Method__product_id\"]=df[\"Query ID\"]+\"_\"+df[\"Ranking Method\"]+\"_\"+df[\"product_id\"]\n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            icc_dict[df[\"User ID\"].iloc[0]+\"_\"+df[\"Query ID\"].iloc[0]+\"_\"+df[\"Ranking Method\"].iloc[0]]=df\n",
    "            \n",
    "        # Add each user's dictionary as a value in a dictionary, creating a nested dictionary.\n",
    "        user_dict[df[\"User ID\"].iloc[0]]=icc_dict\n",
    "    \n",
    "    # Create a list of query IDs.\n",
    "    queries=[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\"]\n",
    "    \n",
    "    # Loop through each query ID for User 3, whose rating file format differs from those of Users 1 and 2.\n",
    "    icc_dict={}\n",
    "    for query in queries:\n",
    "        \n",
    "        # Read each sheet in the rating file as a dataframe and perform basic sanity checks.\n",
    "        df=pd.read_excel(u3_rating_file_path, sheet_name=query)\n",
    "        df.dropna(subset=[\"product_id\",\"review_id\"], inplace=True)\n",
    "\n",
    "        # Separate the dataframe based on ranking method for ease of calculation.\n",
    "        dfa=df.loc[df[\"Ranking Method\"]==\"Average\"].copy()\n",
    "        dfd=df.loc[df[\"Ranking Method\"]==\"Discounted Reward Only\"].copy()\n",
    "        dfda=df.loc[df[\"Ranking Method\"]==\"Discounted Reward with Adjustment\"].copy()\n",
    "\n",
    "        # Create a list of the dataframes corresponding to each ranking method.\n",
    "        df_list=[dfa, dfd, dfda]\n",
    "\n",
    "        # Loop through each dataframe in the list corresponding to a different ranking method.\n",
    "        for dfx in df_list:\n",
    "            \n",
    "            # Keep relevant columns.\n",
    "            dfx=dfx.loc[:,[\"Query ID\", \"Ranking Method\", \"product_id\", \"User ID\", \"Detailed Rating (product level)\"]]\n",
    "            # Create a new column which is a string combination of Query ID, Ranking Method and product_id.\n",
    "            dfx[\"Query ID__Ranking Method__product_id\"]=dfx[\"Query ID\"]+\"_\"+dfx[\"Ranking Method\"]+\"_\"+dfx[\"product_id\"]\n",
    "            # Add relevant information as keys and values in a dictionary.\n",
    "            icc_dict[dfx[\"User ID\"].iloc[0]+\"_\"+dfx[\"Query ID\"].iloc[0]+\"_\"+dfx[\"Ranking Method\"].iloc[0]]=dfx\n",
    "    \n",
    "    # Add User 3's dictionary as a value in the dictionary that contains the result dictionaries for Users 1 and 2.\n",
    "    user_dict[dfx[\"User ID\"].iloc[0]]=icc_dict\n",
    "    \n",
    "    # Combine all 3 users' dataframes by querying the nested dictionary.\n",
    "    main_df=pd.DataFrame()\n",
    "    for user in user_dict.keys():\n",
    "        for df in user_dict[user]:\n",
    "            dfx=user_dict[user][df]\n",
    "            main_df=pd.concat([main_df, dfx])\n",
    "    \n",
    "    # Calculate ICC.\n",
    "    icc=pg.intraclass_corr(data=main_df, targets=\"Query ID__Ranking Method__product_id\", raters=\"User ID\", \n",
    "                           ratings=\"Detailed Rating (product level)\", nan_policy=\"omit\").round(3)\n",
    "    icc.set_index(\"Type\")\n",
    "    \n",
    "    # Return the dataframe containing ICC values.\n",
    "    return icc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed7e6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "icc=ICC(u1_rating_file_path, u2_rating_file_path, u3_rating_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be5d5053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Description</th>\n",
       "      <th>ICC</th>\n",
       "      <th>F</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>pval</th>\n",
       "      <th>CI95%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICC1</td>\n",
       "      <td>Single raters absolute</td>\n",
       "      <td>0.617</td>\n",
       "      <td>5.843</td>\n",
       "      <td>269</td>\n",
       "      <td>540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.56, 0.67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ICC2</td>\n",
       "      <td>Single random raters</td>\n",
       "      <td>0.619</td>\n",
       "      <td>5.993</td>\n",
       "      <td>269</td>\n",
       "      <td>538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.56, 0.68]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICC3</td>\n",
       "      <td>Single fixed raters</td>\n",
       "      <td>0.625</td>\n",
       "      <td>5.993</td>\n",
       "      <td>269</td>\n",
       "      <td>538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.56, 0.68]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ICC1k</td>\n",
       "      <td>Average raters absolute</td>\n",
       "      <td>0.829</td>\n",
       "      <td>5.843</td>\n",
       "      <td>269</td>\n",
       "      <td>540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.79, 0.86]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICC2k</td>\n",
       "      <td>Average random raters</td>\n",
       "      <td>0.830</td>\n",
       "      <td>5.993</td>\n",
       "      <td>269</td>\n",
       "      <td>538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.79, 0.86]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ICC3k</td>\n",
       "      <td>Average fixed raters</td>\n",
       "      <td>0.833</td>\n",
       "      <td>5.993</td>\n",
       "      <td>269</td>\n",
       "      <td>538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.8, 0.86]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Type              Description    ICC      F  df1  df2  pval         CI95%\n",
       "0   ICC1   Single raters absolute  0.617  5.843  269  540   0.0  [0.56, 0.67]\n",
       "1   ICC2     Single random raters  0.619  5.993  269  538   0.0  [0.56, 0.68]\n",
       "2   ICC3      Single fixed raters  0.625  5.993  269  538   0.0  [0.56, 0.68]\n",
       "3  ICC1k  Average raters absolute  0.829  5.843  269  540   0.0  [0.79, 0.86]\n",
       "4  ICC2k    Average random raters  0.830  5.993  269  538   0.0  [0.79, 0.86]\n",
       "5  ICC3k     Average fixed raters  0.833  5.993  269  538   0.0   [0.8, 0.86]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a20de3",
   "metadata": {},
   "source": [
    "The above result table provides the ICC measures for various flavors, the definitions for which are detailed in the [Wikipedia page for ICC](https://en.wikipedia.org/wiki/Intraclass_correlation). In this case, we will be focusing on ICC3k, which is delineated by the following criteria:\n",
    "\n",
    "Two-way mixed: k fixed raters are defined. Each subject is measured by the k raters.\n",
    "Average measures: the reliability is applied to a context where measures of k raters will be averaged for each subject.\n",
    "Consistency: in the context of repeated measurements by the same rater, systematic errors of the rater are cancelled and only the random residual error is kept.\n",
    "\n",
    "For the interpretation of the ICC measure, there are two scales:\n",
    "\n",
    "**Cicchetti scale:**\n",
    "\n",
    "* Less than 0.40: poor.\n",
    "* Between 0.40 and 0.59: fair.\n",
    "* Between 0.60 and 0.74: good.\n",
    "* Above 0.75: excellent.\n",
    "\n",
    "Our ICC value of 0.834 implies that our three raters' inter-rater agreement is \"excellent\" on the Cicchetti scale with the 95% confidence intervals of 0.8 and 0.87 enabling it to stay within that category.\n",
    "\n",
    "**Koo and Li scale:**\n",
    "\n",
    "* Less than 0.50: poor\n",
    "* Between 0.50 and 0.74: moderate\n",
    "* Between 0.75 and 0.89: good\n",
    "* Above 0.90: excellent\n",
    "\n",
    "Our ICC value of 0.834 implies that our three raters' inter-rater agreement is \"good\" on the Koo and Li scale with the 95% confidence intervals of 0.8 and 0.87 enabling it to stay within that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef338a61",
   "metadata": {},
   "source": [
    "### List of libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42e10ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "krippendorff        NA\n",
       "pandas              1.5.3\n",
       "pingouin            0.5.3\n",
       "session_info        1.0.0\n",
       "sklearn             1.2.2\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "PIL                         9.4.0\n",
       "anyio                       NA\n",
       "asttokens                   NA\n",
       "attr                        22.1.0\n",
       "babel                       2.11.0\n",
       "backcall                    0.2.0\n",
       "beta_ufunc                  NA\n",
       "binom_ufunc                 NA\n",
       "bottleneck                  1.3.5\n",
       "brotli                      NA\n",
       "certifi                     2023.07.22\n",
       "cffi                        1.15.1\n",
       "chardet                     4.0.0\n",
       "charset_normalizer          2.0.4\n",
       "cloudpickle                 2.2.1\n",
       "colorama                    0.4.6\n",
       "comm                        0.1.2\n",
       "cycler                      0.10.0\n",
       "cython_runtime              NA\n",
       "cytoolz                     0.12.0\n",
       "dask                        2023.6.0\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.5.1\n",
       "decorator                   5.1.1\n",
       "defusedxml                  0.7.1\n",
       "entrypoints                 0.4\n",
       "executing                   0.8.3\n",
       "fastjsonschema              NA\n",
       "google                      NA\n",
       "hypergeom_ufunc             NA\n",
       "idna                        3.4\n",
       "importlib_metadata          NA\n",
       "importlib_resources         NA\n",
       "invgauss_ufunc              NA\n",
       "ipykernel                   6.19.2\n",
       "ipython_genutils            0.2.0\n",
       "ipywidgets                  8.0.4\n",
       "jedi                        0.18.1\n",
       "jinja2                      3.1.2\n",
       "joblib                      1.2.0\n",
       "json5                       NA\n",
       "jsonschema                  4.17.3\n",
       "jupyter_server              1.23.4\n",
       "jupyterlab_server           2.22.0\n",
       "kiwisolver                  1.4.4\n",
       "littleutils                 NA\n",
       "lxml                        4.9.2\n",
       "lz4                         4.3.2\n",
       "markupsafe                  2.1.1\n",
       "matplotlib                  3.7.1\n",
       "mkl                         2.4.0\n",
       "mpl_toolkits                NA\n",
       "nbformat                    5.7.0\n",
       "nbinom_ufunc                NA\n",
       "ncf_ufunc                   NA\n",
       "nct_ufunc                   NA\n",
       "ncx2_ufunc                  NA\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "numexpr                     2.8.4\n",
       "numpy                       1.24.3\n",
       "openpyxl                    3.0.10\n",
       "outdated                    0.2.2\n",
       "packaging                   23.0\n",
       "pandas_flavor               NA\n",
       "parso                       0.8.3\n",
       "patsy                       0.5.3\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "platformdirs                2.5.2\n",
       "prometheus_client           NA\n",
       "prompt_toolkit              3.0.36\n",
       "psutil                      5.9.0\n",
       "pure_eval                   0.2.2\n",
       "pvectorc                    NA\n",
       "pyarrow                     11.0.0\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.6.0\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.15.1\n",
       "pyparsing                   3.0.9\n",
       "pyrsistent                  NA\n",
       "pythoncom                   NA\n",
       "pytz                        2022.7\n",
       "pywintypes                  NA\n",
       "requests                    2.31.0\n",
       "rfc3339_validator           0.1.4\n",
       "rfc3986_validator           0.1.1\n",
       "scipy                       1.10.1\n",
       "seaborn                     0.12.2\n",
       "send2trash                  NA\n",
       "setuptools                  68.0.0\n",
       "six                         1.16.0\n",
       "skewnorm_ufunc              NA\n",
       "sniffio                     1.2.0\n",
       "socks                       1.7.1\n",
       "sphinxcontrib               NA\n",
       "stack_data                  0.2.0\n",
       "statsmodels                 0.14.0\n",
       "tabulate                    0.8.10\n",
       "tblib                       1.7.0\n",
       "terminado                   0.17.1\n",
       "threadpoolctl               2.2.0\n",
       "tlz                         0.12.0\n",
       "toolz                       0.12.0\n",
       "tornado                     6.2\n",
       "traitlets                   5.7.1\n",
       "typing_extensions           NA\n",
       "urllib3                     1.26.16\n",
       "wcwidth                     0.2.5\n",
       "websocket                   0.58.0\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32con                    NA\n",
       "win32security               NA\n",
       "win32trace                  NA\n",
       "winerror                    NA\n",
       "winpty                      2.0.10\n",
       "xarray                      2023.6.0\n",
       "xxhash                      2.0.2\n",
       "yaml                        6.0\n",
       "zipp                        NA\n",
       "zmq                         23.2.0\n",
       "zoneinfo                    NA\n",
       "zope                        NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.12.0\n",
       "jupyter_client      7.4.9\n",
       "jupyter_core        5.3.0\n",
       "jupyterlab          3.6.3\n",
       "notebook            6.5.4\n",
       "-----\n",
       "Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.22000-SP0\n",
       "-----\n",
       "Session information updated at 2023-08-13 13:40\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "47c21ec07182bd7b8a7046c05381eb5ae64d2b190ea0743ff4dd83b9ddde1114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

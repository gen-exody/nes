{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings \n",
    "\n",
    "In this notebook, we are going to create embeddings on our development and evaluation datasets.  \n",
    "- The development dataset is named *apparel_15to25.tsv.gz* which contains products with 15 to 25 reviews.  \n",
    "- The evaluation dataset is named *apparel_10to14.tsv.gz* which contains products with 10 to 14 reviews.  \n",
    "\n",
    "There are 3 outpus for this notebook. \n",
    "- *apparel_15to25_embedding.pkl* - contains the embeddings for **product_title + review_body** for the development dataset\n",
    "- *apparel_10to14_embedding.pkl* - contains the embeddings for **product_title + review_body** for the evaluation dataset\n",
    "- *apparel_15to25_embedding.pkl* - contains the embeddings for **product_title only** for the development dataset. This will be used for the unsupervised analysis in order to understand what types of products are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import configparser\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your API key from an environment variable or secret management service\n",
    "config = configparser.ConfigParser()\n",
    "config.read('nes.ini')\n",
    "openai.api_key = config['OpenAI']['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the required datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need these columns \n",
    "cols = ['product_id','product_title', 'product_category', 'star_rating', 'review_id', 'review_headline', 'review_body', 'review_length', 'review_count']\n",
    "\n",
    "df_development = pd.read_csv('../resources/data/apparel_15to25.tsv.gz', sep='\\t', compression='gzip')\n",
    "df_development = df_development[cols]\n",
    "\n",
    "df_evaluation = pd.read_csv('../resources/data/apparel_10to14.tsv.gz', sep='\\t', compression='gzip')\n",
    "df_evaluation = df_evaluation[cols]\n",
    "\n",
    "# create the dataset for unsupervised analysis by copying the development dataset\n",
    "df_unsupervised_analysis = df_development.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use product_title and review_body to create the text for search \n",
    "# review_headline tends to be too short or does not provide much context\n",
    "df_development['text'] = df_development['product_title'] + '. ' + df_development['review_body']\n",
    "df_evaluation['text'] = df_evaluation['product_title'] + '. ' + df_evaluation['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding with OpenAI service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function \n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tenacity retry to tackle the OpenAI \"rate limits\" problem\n",
    "# reference: https://platform.openai.com/docs/guides/rate-limits/request-increase\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_embedding_with_backoff(**kwargs):\n",
    "    return get_embedding(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_column_to_embedding(df, source_column, target_column_name, rate_limit_per_minute=3000, delay=60.0):\n",
    "    \n",
    "    num_of_batch = math.ceil(len(df) / rate_limit_per_minute)\n",
    "    # also use batch strategy to handle the OpenAI \"rate limits\" problem apart from the retry mechanism above \n",
    "    chunks = []\n",
    "    tqdm.pandas(desc='Processing rows')\n",
    "    for chunk in np.array_split(df, num_of_batch):\n",
    "        chunk[target_column_name] = chunk[source_column].progress_apply(lambda x:get_embedding_with_backoff(text=x))\n",
    "        chunks.append(chunk)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding for development dataset\n",
    "df_development = transform_column_to_embedding(df=df_development, source_column='text', target_column_name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding for evaluation dataset\n",
    "df_evaluation = transform_column_to_embedding(df=df_evaluation, source_column='text', target_column_name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding for product_title column of the development dataset \n",
    "df_unsupervised_analysis = transform_column_to_embedding(df=df_unsupervised_analysis, source_column='product_title', target_column_name='product_title_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result in pickle format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize to pickle file. Pandas version is 1.3.5 \n",
    "\n",
    "# uncomment below lines to save the datasets\n",
    "#df_development.to_pickle('../resources/data/apparel_15to25_embedding.pkl')\n",
    "#df_evaluation.to_pickle('../resources/data/apparel_10to14_embedding.pkl')\n",
    "#df_unsupervised_analysis.to_pickle('../resources/data/apparel_15to25_product_title_only.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcb8dec5de4f903c075d54d99872f7bf50837ad7ca0b5da4880119383e1aa639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
